# Video Content

The video content for the course can be found as part of multiple playlists from the channel [Artificial Whiteboard](https://www.youtube.com/channel/UCIslxZbyug9pRg00plukOCA) under the playlist [TensorFlow Developer Certificate](https://www.youtube.com/playlist?list=PLcPS3izl8N5sxZa7Ceu4tv8Y7P4TG_mXE).


## [Course Outline (5:21)](https://www.youtube.com/watch?v=D1kAy-pkNp4)    


## [00 - Deep Learning and TensorFlow Fundamentals](https://www.youtube.com/watch?v=YdqgT0IicIw)

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ Chapters ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬
00:00:00 - 00.01 - What is deep learning? (4.38)    
00:04:38 - 00.02 - Why use deep learning? (9.38)    
00:14:17 - 00.03 - What are neural networks? (10.26)    
00:24:43 - 00.04 - What is deep learning already being used for? (8.36)    
00:33:21 - 00.05 - What is, and why use TensorFlow? (7.56)    
00:41:18 - 00.06 - What is a Tensor? (3.37)    
00:44:56 - 00.07 - What we're going to cover throughout the course (4.29)    
00:49:26 - 00.08 - How to approach this course (5.33)    
00:55:00 - 00.09 - Creating your first tensors with TensorFlow and tf.constant() (18.45)    
01:13:46 - 00.10 - Creating tensors with TensorFlow and tf.Variable() (7.07)    
01:20:54 - 00.11 - Creating random tensors with TensorFlow (9.40)    
01:30:35 - 00.12 - Shuffling the order of tensors (9.40)    
01:40:16 - 00.13 - Creating tensors from NumPy arrays (11.55)    
01:52:12 - 00.14 - Getting information from your tensors (tensor attributes) (11.57)    
02:04:10 - 00.15 - Indexing and expanding tensors (12.33)    
02:16:44 - 00.16 - Manipulating tensors with basic operations (5.34)    
02:22:19 - 00.17 - Matrix multiplication with tensors - Part 1 (11.53)    
02:34:14 - 00.18 - Matrix multiplication with tensors - Part 2 (13.29)    
02:47:44 - 00.19 - Matrix multiplication with tensors - Part 3 (10.03)    
02:57:47 - 00.20 - Changing the datatype of tensors (6.55)    
03:04:44 - 00.21 - Tensor aggregation (finding the min, max, mean & more) (9.49)    
03:14:13 - 00.22 - Tensor troubleshooting example (updating tensor data types) (6.13)    
03:20:47 - 00.23 - Finding the positional minimum and maximum of a tensor (argmin and argmax) (9.31)    
03:30:19 - 00.24 - Squeezing a tensor (removing all 1-dimension axes) (2.59)    
03:33:19 - 00.25 - One-hot encoding tensors (5.46)    
03:39:06 - 00.26 - Trying out more tensor math operations (4.47)    
03:43:54 - 00.27 - Exploring TensorFlow and NumPy's compatibility (5.43)    
03:49:37 - 00.28 - Making sure our tensor operations run really fast on GPUs (10.19)    


## [01 - Neural Network Regression with TensorFlow](https://www.youtube.com/watch?v=jLDq7yeLVQ0)

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ Chapters ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬
00:00:00 - 01.01 - Introduction to Neural Network Regression with TensorFlow (7.33)    
00:07:34 - 01.02 - Inputs and outputs of a neural network regression model (8.59)    
00:16:33 - 01.03 - Anatomy and architecture of a neural network regression model (7.55)    
00:24:28 - 01.04 - Creating sample regression data (so we can model it) (12.46)    
00:37:16 - 01.05 - The major steps in modelling with TensorFlow (20.16)    
00:57:33 - 01.06 - Steps in improving a model with TensorFlow - Part 1 (6.02)    
00:07:34 - 01.07 - Steps in improving a model with TensorFlow - Part 2 (9.25)    
01:03:36 - 01.08 - Steps in improving a model with TensorFlow - Part 3 (12.33)    
01:13:02 - 01.09 - Evaluating a TensorFlow model - Part 1 ("visualize, visualize, visualize") (7.24)    
01:25:36 - 01.10 - Evaluating a TensorFlow model - Part 2 (the three datasets) (11.01)    
01:33:00 - 01.11 - Evaluating a TensorFlow model - Part 3 (getting a model summary) (17.18)    
01:44:03 - 01.12 - Evaluating a TensorFlow model - Part 4 (visualizing a model's layers) (7.14)    
02:01:23 - 01.13 - Evaluating a TensorFlow model - Part 5 (visualizing a model's predictions) (9.16)    
02:08:38 - 01.14 - Evaluating a TensorFlow model - Part 6 (common regression evaluation metrics) (8.05)    
02:17:55 - 01.15 - Evaluating a TensorFlow regression model - Part 7 (mean absolute error) (5.52)    
02:26:00 - 01.16 - Evaluating a TensorFlow regression model - Part 8 (mean square error) (3.18)    
02:31:54 - 01.17 - Setting up TensorFlow modelling experiments - Part 1 (start with a simple model) (13.50)    
02:49:04 - 01.18 - Setting up TensorFlow modelling experiments - Part 2 (increasing complexity) (11.29)    
03:00:36 - 01.19 - Comparing and tracking your TensorFlow modelling experiments (10.20)    
03:10:55 - 01.20 - How to save a TensorFlow model (8.19)    
03:19:16 - 01.21 - How to load and use a saved TensorFlow model (10.15)    
03:29:32 - 01.22 - (Optional) How to save and download files from Google Colab (6.18)    
03:35:51 - 01.23 - Putting together what we've learned - Part 1 (preparing a dataset) (13.31)    
03:49:23 - 01.24 - Putting together what we've learned - Part 2 (building a regression model) (13.20)    
04:02:45 - 01.25 - Putting together what we've learned - Part 3 (improving our regression model) (15.47)    
04:18:33 - 01.26 - Preprocessing data with feature scaling - Part 1 (what is feature scaling?) (9.34)    
04:28:08 - 01.27 - Preprocessing data with feature scaling - Part 2 (normalizing our data) (10.57)    
04:39:05 - 01.28 - Preprocessing data with feature scaling - Part 3 (fitting a model on scaled data) (7.40)    


## [02 - Neural Network Classification in TensorFlow](https://www.youtube.com/watch?v=aTw053kfu5Q)

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ Chapters ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬
00:00:00 - 02.01 - Introduction to neural network classification in TensorFlow (8.25)    
00:08:26 - 02.02 - Example classification problems (and their inputs and outputs) (6.38)    
00:15:04 - 02.03 - Input and output tensors of classification problems (6.21)    
00:21:26 - 02.04 - Typical architecture of neural network classification models with TensorFlow (9.36)    
00:31:03 - 02.05 - Creating and viewing classification data to model (11.34)    
00:42:38 - 02.06 - Checking the input and output shapes of our classification data (4.38)    
00:47:16 - 02.07 - Building a not very good classification model with TensorFlow (12.10)    
00:59:28 - 02.08 - Trying to improve our not very good classification model (9.13)    
01:08:42 - 02.09 - Creating a function to view our model's not so good predictions (15.08)    
01:23:51 - 02.10 - Make our poor classification model work for a regression dataset (24.36)    
01:36:10 - 02.11 - Non-linearity - Part 1 (Straight lines and non-straight lines) (9.38)    
01:45:50 - 02.12 - Non-linearity - Part 2 (Building our first neural network with non-linearity) (5.47)    
01:51:38 - 02.13 - Non-linearity - Part 3 (Upgrading our non-linear model with more layers) (10.18)    
02:01:57 - 02.14 - Non-linearity - Part 4 (Modelling our non-linear data once and for all) (8.37)    
02:10:35 - 02.15 - Non-linearity - Part 5 (Replicating non-linear activation functions from scratch) (14.26)    
02:25:03 - 02.16 - Getting great results in less time by tweaking the learning rate (14.47)    
02:39:51 - 02.17 - Using the TensorFlow History object to plot a model's loss curves (6.11)    
02:46:04 - 02.18 - Using callbacks to find a model's ideal learning rate (17.32)    
03:03:37 - 02.19 - Training and evaluating a model with an ideal learning rate (9.20)    
03:12:58 - 02.20 - Introducing more classification evaluation methods (6.04)    
03:19:03 - 02.21 - Finding the accuracy of our classification model (4.17)    
03:23:21 - 02.22 - Creating our first confusion matrix (to see where our model is getting confused) (8.27)    
03:31:50 - 02.23 - Making our confusion matrix prettier (14.00)    
03:45:51 - 02.24 - Multi-class classification - Part 1 (Getting the data) (10.37)    
03:56:29 - 02.25 - Multi-class classification - Part 2 (Becoming one with the data) (7.07)    
04:03:38 - 02.26 - Multi-class classification - Part 3 (Building a multi-class classification model) (15.38)    
04:19:17 - 02.27 - Multi-class classification - Part 4 (Improving performance with normalization) (12.43)    
04:32:01 - 02.28 - Multi-class classification - Part 5 (Comparing normalized and non-normalised data) (4.13)    
04:36:15 - 02.29 - Multi-class classification - Part 6 (Finding the ideal learning rate) (10.38)    
04:46:54 - 02.30 - Multi-class classification - Part 7 (Evaluating our model) (13.16)    
05:00:11 - 02.31 - Multi-class classification - Part 8 (Creating a confusion matrix) (4.26)    
05:04:38 - 02.32 - Multi-class classification - Part 9 (Visualizing random model predictions) (10.42)    
05:15:20 - 02.33 - What "patterns" is our model learning? (15.33)    


## [03 - Computer Vision and Convolutional Neural Networks in TensorFlow](https://www.youtube.com/watch?v=i-P3w7QDN-8)

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ Chapters ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬
00:00:00 - 03.01 - Introduction to Computer Vision with TensorFlow (9.36)    
00:09:36 - 03.02 - Introduction to Convolutional Neural Networks (CNNs) with TensorFlow (7.59)    
00:17:36 - 03.03 - Downloading an image dataset for our first Food Vision model (8.27)    
00:26:04 - 03.04 - Becoming One With Data - Part 1 (Inspecting the File paths) (5.05)    
00:31:10 - 03.05 - Becoming One With Data - Part 2 (12.26)    
00:43:37 - 03.06 - Becoming One With Data - Part 3 (4.22)    
00:48:00 - 03.07 - Building an end to end CNN Model (18.17)    
01:06:19 - 03.08 - Using a GPU to run our CNN model 5x faster (9.17)    
01:15:36 - 03.09 - Trying a non-CNN model on our image data (8.51)    
01:24:28 - 03.10 - Improving our non-CNN model by adding more layers (9.52)    
01:34:21 - 03.11 - Breaking our CNN model down - Part 1 (Becoming one with the data) (9.03)    
01:43:25 - 03.12 - Breaking our CNN model down - Part 2 (Preparing to load our data) (11.46)    
01:55:12 - 03.13 - Breaking our CNN model down - Part 3 (Loading our data with ImageDataGenerator) (9.54)    
02:05:07 - 03.14 - Breaking our CNN model down - Part 4 (Building a baseline CNN model (8.02)    
02:13:10 - 03.15 - Breaking our CNN model down - Part 5 (Looking inside a Conv2D layer) (15.20)    
02:28:32 - 03.16 - Breaking our CNN model down - Part 6 (Compiling and fitting our baseline CNN) (7.14)    
02:35:47 - 03.17 - Breaking our CNN model down - Part 7 (Evaluating our CNN's training curves) (11.45)    
02:47:33 - 03.18 - Breaking our CNN model down - Part 8 (Reducing overfitting with Max Pooling) (13.40)    
03:01:14 - 03.19 - Breaking our CNN model down - Part 9 (Reducing overfitting with data augmentation) (6.52)    
03:08:06 - 03.20 - Breaking our CNN model down - Part 10 (Visualizing our augmented data) (15.04)    
03:23:11 - 03.21 - Breaking our CNN model down - Part 11 (Training a CNN model on augmented data) (8.49)    
03:32:01 - 03.22 - Breaking our CNN model down - Part 12 (Discovering the power of shuffling data) (10.01)    
03:42:03 - 03.23 - Breaking our CNN model down - Part 13 (Exploring options to improve our model) (5.21)    
03:47:26 - 03.24 - Downloading a custom image to make predictions on (4.54)    
03:52:20 - 03.25 - Writing a helper function to load and preprocessing custom images (10.00)    
04:02:22 - 03.26 - Making a prediction on a custom image with our trained CNN (10.08)    
04:12:31 - 03.27 - Multi-class CNN's - Part 1 (Becoming one with the data) (14.59)    
04:27:31 - 03.28 - Multi-class CNN's - Part 2 (Preparing our data, turning it into tensors) (6.38)    
04:34:10 - 03.29 - Multi-class CNN's - Part 3 (Building a multi-class CNN model) (7.24)    
04:41:35 - 03.30 - Multi-class CNN's - Part 4 (Fitting a multi-class CNN model to the data) (6.02)    
04:47:38 - 03.31 - Multi-class CNN's - Part 5 (Evaluating our multi-class CNN model) (4.51)    
04:52:30 - 03.32 - Multi-class CNN's - Part 6 (Trying to fix overfitting by removing layers) (12.19)    
05:04:50 - 03.33 - Multi-class CNN's - Part 7 (Trying to fix overfitting with data augmentation) (11.45)    
05:16:37 - 03.34 - Multi-class CNN's - Part 8 (Things you could do to improve your CNN model) (4.23)    
05:21:01 - 03.35 - Multi-class CNN's - Part 9 (Making predictions with our model on custom images) (9.22)    
05:30:24 - 03.36 - Saving and loading our trained CNN model (6.21)    


## [04 - Transfer Learning in TensorFlow - Part 1 - Feature Extraction](https://www.youtube.com/watch?v=E799buwqA-k)

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ Chapters ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬
00:00:00 - 04.01 - What is and, why use transfer learning? (10.12)    
00:10:13 - 04.02 - Downloading and preparing data for our first transfer learning model (14.39)    
00:24:53 - 04.03 - Introducing Callbacks in TensorFlow and making a callback to track our models (10.01)    
00:34:55 - 04.04 - Exploring the TensorFlow Hub website for pretrained models (9.51)    
00:44:47 - 04.05 - Building and compiling a TensorFlow Hub feature extraction model (14.00)    
00:58:48 - 04.06 - Blowing our previous models out of the water with transfer learning (9.13)    
01:08:02 - 04.07 - Plotting the loss curves of our ResNet feature extraction model (7.35)    
01:15:38 - 04.08 - Building and training a pretrained EfficientNet model on our data (9.42)    
01:25:21 - 04.09 - Different Types of Transfer Learning (11.40)    
01:37:02 - 04.10 - Comparing Our Model's Results (15.16)    


## [05 - Transfer Learning in TensorFlow - Part 2 (Fine-Tuning)](https://www.youtube.com/watch?v=biAIFUL7UuQ)

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ Chapters ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬
00:00:00 - 05.01 - Introduction to Transfer Learning in TensorFlow - Part 2 (Fine-Tuning) (6.16)    
00:06:16 - 05.02 - Importing a script full of helper functions (and saving lots of space) (7.35)    
00:13:52 - 05.03 - (Excercise) Imposter Syndrome (2.55)    
00:16:48 - 05.04 - Downloading and turning our images into a TensorFlow BatchDataset (15.38)    
00:32:28 - 05.05 - Discussing the four (actually five) modelling experiments we're running (2.15)    
00:34:43 - 05.06 - Comparing the TensorFlow Keras Sequential API versus the Functional API (2.34)    
00:37:17 - 05.07 - Creating our first model with the TensorFlow Keras Functional API (11.38)    
00:48:57 - 05.08 - Compiling and fitting our first Functional API model (10.53)    
00:59:51 - 05.09 - Getting a feature vector from our trained model (13.39)    
01:13:31 - 05.10 - Drilling into the concept of a feature vector (a learned representation) (3.43)    
01:17:15 - 05.11 - Downloading and preparing the data for Model 1 (1 percent of training data) (9.51)    
01:27:07 - 05.12 - Building a data augmentation layer to use inside our model (12.06)    
01:39:14 - 05.13 - Visualizing what happens when images pass through our data augmentation layer (10.56)    
01:50:10 - 05.14 - Building Model 1 (with a data augmentation layer and 1% of training data) (15.55)    
02:06:06 - 05.15 - Building Model 2 (with a data augmentation layer and 10% of training data) (16.37)    
02:22:45 - 05.16 - Creating a ModelCheckpoint to save our model's weights during training (7.25)    
02:30:10 - 05.17 - Fitting and evaluating Model 2 (and saving its weights using ModelCheckpoint) (7.14)    
02:37:25 - 05.18 - Loading and comparing saved weights to our existing trained Model 2 (7.17)    
02:44:43 - 05.19 - Preparing Model 3 (our first fine-tuned model) (20.26)    
03:05:11 - 05.20 - Fitting and evaluating Model 3 (our first fine-tuned model) (7.45)    
03:12:57 - 05.21 - Comparing our model's results before and after fine-tuning (10.26)    
03:23:25 - 05.22 - Downloading and preparing data for our biggest experiment yet (Model 4) (6.24)    
03:29:50 - 05.23 - Preparing our final modelling experiment (Model 4) (12.00)    
03:41:51 - 05.24 - Fine-tuning Model 4 on 100% of the training data and evaluating its results (10.19)    
03:52:11 - 05.25 - Comparing our modelling experiment results in TensorBoard (10.46)    
04:02:58 - 05.26 - How to view and delete previous TensorBoard experiments (2.04)    


## [06 - Transfer Learning in TensorFlow - Part 3 (Scaling Up)](https://www.youtube.com/watch?v=0KXMNLZd9ZY)

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ Chapters ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬
00:00:00 - 06.01 - Introduction to Transfer Learning - Part 3 (Scaling Up) (6.19)    
00:06:20 - 06.02 - Getting helper functions ready and downloading data to model (13.34)    
00:19:55 - 06.03 - Outlining the model we're going to build and building a ModelCheckpoint callback (5.38)    
00:25:34 - 06.04 - Creating a data augmentation layer to use with our model (4.39)    
00:30:14 - 06.05 - Creating a headless EfficientNetB0 model with data augmentation built in (8.58)    
00:39:13 - 06.06 - Fitting and evaluating our biggest transfer learning model yet (7.56)    
00:47:10 - 06.07 - Unfreezing some layers in our base model to prepare for fine-tuning (11.28)    
00:58:39 - 06.08 - Fine-tuning our feature extraction model and evaluating its performance (8.23)    
01:07:31 - 06.09 - Saving and loading our trained model (6.25)    
01:13:30 - 06.10 - Downloading a pretrained model to make and evaluate predictions with (6.34)    
01:20:05 - 06.11 - Making predictions with our trained model on 25,250 test samples (12.46)    
01:32:52 - 06.12 - Unravelling our test dataset for comparing ground truth labels to predictions (6.05)    
01:38:57 - 06.13 - Confirming our model's predictions are in the same order as the test labels (5.17)    
01:44:15 - 06.14 - Creating a confusion matrix for our model's 101 different classes (12.07)    
01:56:23 - 06.15 - Evaluating every individual class in our dataset (14.16)    
02:10:41 - 06.16 - Plotting our model's F1-scores for each separate class (7.36)    
02:18:17 - 06.17 - Creating a function to load and prepare images for making predictions (12.08)    
02:30:27 - 06.18 - Making predictions on our test images and evaluating them (16.06)    
02:46:34 - 06.19 - Discussing the benefits of finding your model's most wrong predictions (6.09)    
02:52:44 - 06.20 - Writing code to uncover our model's most wrong predictions (11.16)    
03:04:01 - 06.21 - Plotting and visualizing the samples our model got most wrong (10.36)    
03:14:38 - 06.22 - Making predictions on and plotting our own custom images (9.49)    


## [07 - Milestone Project 1 - Food Vision Big™](https://www.youtube.com/watch?v=DHAliaKd-mk)

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ Chapters ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬
00:00:00 - 07.01 - Introduction to Milestone Project 1 (Food Vision Big™) (5.44)    
00:05:44 - 07.02 - Making sure we have access to the right GPU for mixed precision training (10.17)    
00:16:02 - 07.03 - Getting helper functions ready (3.06)    
00:19:09 - 07.04 - Introduction to TensorFlow Datasets (TFDS) (12.03)    
00:31:13 - 07.05 - Exploring and becoming one with the data (Food101 from TensorFlow Datasets) (15.56)    
00:47:10 - 07.06 - Creating a preprocessing function to prepare our data for modelling (15.50)    
01:03:01 - 07.07 - Batching and preparing our datasets (to make them run fast) (13.47)    
01:16:49 - 07.08 - Exploring what happens when we batch and prefetch our data (6.49)    
01:23:39 - 07.09 - Creating modelling callbacks for our feature extraction model (7.14)    
01:30:54 - 07.10 - Turning on mixed precision training with TensorFlow (10.05)    
01:40:59 - 07.11 - Creating a feature extraction model capable of using mixed precision training (12.42)    
01:53:43 - 07.12 - Checking to see if our model is using mixed precision training layer by layer (7.56)    
02:01:40 - 07.13 - Training and evaluating a feature extraction model (Food Vision Big™) (10.19)    
02:12:00 - 07.14 - Introducing your Milestone Project 1 challenge (Build a model to beat DeepFood) (7.47)    


## 08 - NLP Fundamentals in TensorFlow

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ Chapters ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬    
08.01 - Introduction to Natural Language Processing (NLP) and Sequence Problems (12.51)    
08.02 - Example NLP inputs and outputs (7.22)    
08.03 - The typical architecture of a Recurrent Neural Network (RNN) (9.03)    
08.04 - Preparing a notebook for our first NLP with TensorFlow project (8.52)    
08.05 - Becoming one with the data and visualizing a text dataset (16.41)    
08.06 - Splitting data into training and validation sets (6.26)    
08.07 - Converting text data to numbers using tokenisation and embeddings (overview) (9.22)    
08.08 - Setting up a TensorFlow TextVectorization layer to convert text to numbers (17.10)    
08.09 - Mapping the TextVectorization layer to text data and turning it into numbers (11.02)    
08.10 - Creating an Embedding layer to turn tokenised text into embedding vectors (12.27)    
08.11 - Discussing the various modelling experiments we're going to run (8.57)    
08.12 - Model 0: Building a baseline model to try and improve upon (9.25)    
08.13 - Creating a function to track and evaluate our model's results (12.14)    
08.14 - Model 1: Building, fitting and evaluating our first deep model on text data (20.51)    
08.15 - Visualizing our model's learned word embeddings with TensorFlow's projector tool (20.43)    
08.16 - High-level overview of Recurrent Neural Networks (RNNs) + where to learn more (9.34)    
08.17 - Model 2: Building, fitting and evaluating our first TensorFlow RNN model (LSTM) (18.16)    
08.18 - Model 3: Building, fitting and evaluating a GRU-cell powered RNN (16.56)    
08.19 - Model 4: Building, fitting and evaluating a bidirectional RNN model (19.34)    
08.20 - Discussing the intuition behind Conv1D neural networks for text and sequences (19.31)    
08.21 - Model 5: Building, fitting and evaluating a 1D CNN for text (9.57)    
08.22 - Using TensorFlow Hub for pretrained word embeddings (transfer learning for NLP) (13.45)    
08.23 - Model 6: Building, training and evaluating a transfer learning model for NLP (10.45)    
08.24 - Preparing subsets of data for model 7 (same as model 6 but 10% of data) (10.52)    
08.25 - Model 7: Building, training and evaluating a transfer learning model on 10% data (10.04)    
08.26 - Fixing our data leakage issue with model 7 and retraining it (13.42)    
08.27 - Comparing all our modelling experiments evaluation metrics (13.14)    
08.28 - Uploading our model's training logs to TensorBoard and comparing them (11.14)    
08.29 - Uploading our model's training logs to TensorBoard and comparing them (11.14)    
08.30 - Downloading a pretrained model and preparing data to investigate predictions (13.24)    
08.31 - Visualizing our model's most wrong predictions (8.28)    
08.32 - Making and visualizing predictions on the test dataset (8.27)    
08.33 - Understanding the concept of the speed/score tradeoff (15.01)    
