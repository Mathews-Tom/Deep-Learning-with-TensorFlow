# Video Content

The video content for the course can be found as part of multiple playlists from the channel [Artificial Whiteboard](https://www.youtube.com/channel/UCIslxZbyug9pRg00plukOCA). Playlists can be found as seperate playlists based on the different sections as given below or a single uber playlist [TensorFlow Developer Certificate in 2022: Zero to Mastery](https://www.youtube.com/playlist?list=PLcPS3izl8N5sqopxoCdeY0OWPU13Jqlon) with all the video content for anyone to work their way towards taking the TensorFlow Developer Certificate.

## [Course Outline (5:21)](https://www.youtube.com/watch?v=D1kAy-pkNp4&list=PLcPS3izl8N5sqopxoCdeY0OWPU13Jqlon)

## 00 - Deep Learning and TensorFlow Fundamentals
- [00.01 - What is deep learning? (4:38)]
- [00.02 - Why use deep learning? (9:38)]
- [00.03 - What are neural networks? (10:26)]
- [00.04 - What is deep learning already being used for? (8:36)]
- [00.05 - What is and why use TensorFlow? (7:56)]
- [00.06 - What is a Tensor? (3:37)]
- [00.07 - What we're going to cover throughout the course (4:29)]
- [00.08 - How to approach this course (5:33)]
- [00.09 - Creating your first tensors with TensorFlow and tf.constant() (18:45)]
- [00.10 - Creating tensors with TensorFlow and tf.Variable() (7:07)]
- [00.11 - Creating random tensors with TensorFlow (9:40)]
- [00.12 - Shuffling the order of tensors (9:40)])
- [00.13 - Creating tensors from NumPy arrays (11:55)]
- [00.14 - Getting information from your tensors (tensor attributes) (11:57)]
- [00.15 - Indexing and expanding tensors (12:33)]
- [00.16 - Manipulating tensors with basic operations (5:34)]
- [00.17 - Matrix multiplication with tensors - Part 1 (11:53)]
- [00.18 - Matrix multiplication with tensors - Part 2 (13:29)]
- [00.19 - Matrix multiplication with tensors - Part 3 (10:03)]
- [00.20 - Changing the datatype of tensors (6:55)]
- [00.21 - Tensor aggregation (finding the min, max, mean & more) (9:49)]
- [00.22 - Tensor troubleshooting example (updating tensor datatypes) (6:13)]
- [00.23 - Finding the positional minimum and maximum of a tensor (argmin and argmax) (9:31)]
- [00.24 - Squeezing a tensor (removing all 1-dimension axes) (2:59)])
- [00.25 - One-hot encoding tensors (5:46)]
- [00.26 - Trying out more tensor math operations (4:47)]
- [00.27 - Exploring TensorFlow and NumPy's compatibility (5:43)]
- [00.28 - Making sure our tensor operations run really fast on GPUs (10:19)]

## 01 - Neural Network Regression with TensorFlow
- [01.01 - Introduction to Neural Network Regression with TensorFlow (7:33)]
- [01.02 - Inputs and outputs of a neural network regression model (8:59)]
- [01.03 - Anatomy and architecture of a neural network regression model (7:55)]
- [01.04 - Creating sample regression data (so we can model it) (12:46)]
- [01.05 - The major steps in modelling with TensorFlow (40:30)])
- [01.06 - Steps in improving a model with TensorFlow - Part 1 (6:02)])
- [01.07 - Steps in improving a model with TensorFlow - Part 2 (9:25)])
- [01.08 - Steps in improving a model with TensorFlow - Part 3 (12:33)])
- [01.09 - Evaluating a TensorFlow model - Part 1 ("visualise, visualise, visualise") (7:24)]
- [01.10 - Evaluating a TensorFlow model - Part 2 (the three datasets) (11:01)]
- [01.11 - Evaluating a TensorFlow model - Part 3 (getting a model summary) (17:18)])
- [01.12 - Evaluating a TensorFlow model - Part 4 (visualising a model's layers) (7:14)]
- [01.13 - Evaluating a TensorFlow model - Part 5 (visualising a model's predictions) (9:16)]
- [01.14 - Evaluating a TensorFlow model - Part 6 (common regression evaluation metrics) (8:05)])
- [01.15 - Evaluating a TensorFlow regression model - Part 7 (mean absolute error) (5:52)])
- [01.16 - Evaluating a TensorFlow regression model - Part 8 (mean square error) (3:18)]
- [01.17 - Setting up TensorFlow modelling experiments - Part 1 (start with a simple model) (13:50)]
- [01.18 - Setting up TensorFlow modelling experiments - Part 2 (increasing complexity) (11:29)]
- [01.19 - Comparing and tracking your TensorFlow modelling experiments (10:20)]
- [01.20 - How to save a TensorFlow model (8:19)])
- [01.21 - How to load and use a saved TensorFlow model (10:15)]
- [01.22 - (Optional) How to save and download files from Google Colab (6:18)]
- [01.23 - Putting together what we've learned - Part 1 (preparing a dataset) (13:31)]
- [01.24 - Putting together what we've learned - Part 2 (building a regression model) (13:20)]
- [01.25 - Putting together what we've learned - Part 3 (improving our regression model) (15:47)]
- [01.26 - Preprocessing data with feature scaling - Part 1 (what is feature scaling?) (9:34)]
- [01.27 - Preprocessing data with feature scaling - Part 2 (normalising our data) (10:57)]
- [01.28 - Preprocessing data with feature scaling - Part 3 (fitting a model on scaled data) (7:40)]

## 02 - Neural network classification in TensorFlow
- [02.01 - Introduction to neural network classification in TensorFlow (8:25)])
- [02.02 - Example classification problems (and their inputs and outputs) (6:38)]
- [02.03 - Input and output tensors of classification problems (6:21)]
- [02.04 - Typical architecture of neural network classification models with TensorFlow (9:36)]
- [02.05 - Creating and viewing classification data to model (11:34)]
- [02.06 - Checking the input and output shapes of our classification data (4:38)]
- [02.07 - Building a not very good classification model with TensorFlow (12:10)]
- [02.08 - Trying to improve our not very good classification model (9:13)])
- [02.09 - Creating a function to view our model's not so good predictions (15:08)])
- [02.10 - Make our poor classification model work for a regression dataset (24:36)]
- [02.11 - Non-linearity - Part 1 (Straight lines and non-straight lines) (9:38)]
- [02.12 - Non-linearity - Part 2 (Building our first neural network with non-linearity) (5:47)]
- [02.13 - Non-linearity - Part 3 (Upgrading our non-linear model with more layers) (10:18)]
- [02.14 - Non-linearity - Part 4 (Modelling our non-linear data once and for all) (8:37)])
- [02.15 - Non-linearity - Part 5 (Replicating non-linear activation functions from scratch) (14:26)]
- [02.16 - Getting great results in less time by tweaking the learning rate (14:47)]
- [02.17 - Using the TensorFlow History object to plot a model's loss curves (6:11)]
- [02.18 - Using callbacks to find a model's ideal learning rate (17:32)]
- [02.19 - Training and evaluating a model with an ideal learning rate (9:20)]
- [02.20 - Introducing more classification evaluation methods (6:04)])
- [02.21 - Finding the accuracy of our classification model (4:17)]
- [02.22 - Creating our first confusion matrix (to see where our model is getting confused) (8:27)]
- [02.23 - Making our confusion matrix prettier (14:00)]
- [02.24 - Multi-class classification - Part 1 (Getting the data) (10:37)]
- [02.25 - Multi-class classification - Part 2 (Becoming one with the data) (7:07)]
- [02.26 - Multi-class classification - Part 3 (Building a multi-class classification model) (15:38)])
- [02.27 - Multi-class classification - Part 4 (Improving performance with normalisation) (12:43)]
- [02.28 - Multi-class classification - Part 5 (Comparing normalised and non-normalised data) (4:13)]
- [02.29 - Multi-class classification - Part 6 (Finding the ideal learning rate) (10:38)]
- [02.30 - Multi-class classification - Part 7 (Evaluating our model) (13:16)]
- [02.31 - Multi-class classification - Part 8 (Creating a confusion matrix) (4:26)])
- [02.32 - Multi-class classification - Part 9 (Visualising random model predictions) (10:42)]
- [02.33 - What "patterns" is our model learning? (15:33)]

## 03 - Computer Vision and Convolutional Neural Networks in TensorFlow
- [03.01 - Introduction to Computer Vision with TensorFlow (9:36)]
- [03.02 - Introduction to Convolutional Neural Networks (CNNs) with TensorFlow (7:59)]
- [03.03 - Downloading an image dataset for our first Food Vision model (8:27)]
- [03.04 - Becoming One With Data - Part 1 (Inspecting the File paths) (5:05)])
- [03.05 - Becoming One With Data - Part 2 (12:26)]
- [03.06 - Becoming One With Data - Part 3 (4:22)]
- [03.07 - Building an end to end CNN Model (18:17)]
- [03.08 - Using a GPU to run our CNN model 5x faster (9:17)]
- [03.09 - Trying a non-CNN model on our image data (8:51)]
- [03.10 - Improving our non-CNN model by adding more layers (9:52)]
- [03.11 - Breaking our CNN model down - Part 1 (Becoming one with the data) (9:03)]
- [03.12 - Breaking our CNN model down - Part 2 (Preparing to load our data) (11:46)]
- [03.13 - Breaking our CNN model down - Part 3 (Loading our data with ImageDataGenerator) (9:54)]
- [03.14 - Breaking our CNN model down - Part 4 (Building a baseline CNN model (8:02)]
- [03.15 - Breaking our CNN model down - Part 5 (Looking inside a Conv2D layer) (15:20)]
- [03.16 - Breaking our CNN model down - Part 6 (Compiling and fitting our baseline CNN) (7:14)]
- [03.17 - Breaking our CNN model down - Part 7 (Evaluating our CNN's training curves) (11:45)]
- [03.18 - Breaking our CNN model down - Part 8 (Reducing overfitting with Max Pooling) (13:40)]
- [03.19 - Breaking our CNN model down - Part 9 (Reducing overfitting with data augmentation) (6:52)]
- [03.20 - Breaking our CNN model down - Part 10 (Visualizing our augmented data) (15:04)]
- [03.21 - Breaking our CNN model down - Part 11 (Training a CNN model on augmented data) (8:49)]
- [03.22 - Breaking our CNN model down - Part 12 (Discovering the power of shuffling data) (10:01)]
- [03.23 - Breaking our CNN model down - Part 13 (Exploring options to improve our model) (5:21)]
- [03.24 - Downloading a custom image to make predictions on (4:54)]
- [03.25 - Writing a helper function to load and preprocessing custom images (10:00)]
- [03.26 - Making a prediction on a custom image with our trained CNN (10:08)]
- [03.27 - Multi-class CNN's - Part 1 (Becoming one with the data) (14:59)]
- [03.28 - Multi-class CNN's - Part 2 (Preparing our data, turning it into tensors) (6:38)]
- [03.29 - Multi-class CNN's - Part 3 (Building a multi-class CNN model) (7:24)]
- [03.30 - Multi-class CNN's - Part 4 (Fitting a multi-class CNN model to the data) (6:02)]
- [03.31 - Multi-class CNN's - Part 5 (Evaluating our multi-class CNN model) (4:51)]
- [03.32 - Multi-class CNN's - Part 6 (Trying to fix overfitting by removing layers) (12:19)]
- [03.33 - Multi-class CNN's - Part 7 (Trying to fix overfitting with data augmentation) (11:45)]
- [03.34 - Multi-class CNN's - Part 8 (Things you could do to improve your CNN model) (4:23)]
- [03.35 - Multi-class CNN's - Part 9 (Making predictions with our model on custom images) (9:22)]
- [03.36 - Saving and loading our trained CNN model (6:21)]

## 04 - Transfer Learning in TensorFlow - Part 1 :: Feature Extraction
- [04.01 - What is and why use transfer learning? (10:12)]
- [04.02 - Downloading and preparing data for our first transfer learning model (14:39)]
- [04.03 - Introducing Callbacks in TensorFlow and making a callback to track our models (10:01)]
- [04.04 - Exploring the TensorFlow Hub website for pretrained models (9:51)]
- [04.05 - Building and compiling a TensorFlow Hub feature extraction model (14:00)]
- [04.06 - Blowing our previous models out of the water with transfer learning (9:13)]
- [04.07 - Plotting the loss curves of our ResNet feature extraction model (7:35)]
- [04.08 - Building and training a pre-trained EfficientNet model on our data (9:42)]
- [04.09 - Different Types of Transfer Learning (11:40)]
- [04.10 - Comparing Our Model's Results (15:16)]

## 05 - Transfer Learning in TensorFlow - Part 2 :: Fine Tuning
- [05.01 - Introduction to Transfer Learning in TensorFlow Part 2: Fine-tuning (6:16)]
- [05.02 - Importing a script full of helper functions (and saving lots of space) (7:35)]
- [05.03 - (Excercise) Imposter Syndrome (2:55)]
- [05.04 - Downloading and turning our images into a TensorFlow BatchDataset (15:38)]
- [05.05 - Discussing the four (actually five) modelling experiments we're running (2:15)]
- [05.06 - Comparing the TensorFlow Keras Sequential API versus the Functional API (2:34)]
- [05.07 - Creating our first model with the TensorFlow Keras Functional API (11:38)]
- [05.08 - Compiling and fitting our first Functional API model (10:53)]
- [05.09 - Getting a feature vector from our trained model (13:39)]
- [05.10 - Drilling into the concept of a feature vector (a learned representation) (3:43)]
- [05.11 - Downloading and preparing the data for Model 1 (1 percent of training data) (9:51)]
- [05.12 - Building a data augmentation layer to use inside our model (12:06)]
- [Visualizing what happens when images pass through our data augmentation layer (21:50)]
- [Building Model 1 (with a data augmentation layer and 1% of training data) (15:55)]
- [Building Model 2 (with a data augmentation layer and 10% of training data) (16:37)]
- [Creating a ModelCheckpoint to save our model's weights during training (7:25)]
- [Fitting and evaluating Model 2 (and saving its weights using ModelCheckpoint) (7:14)]
- [Loading and comparing saved weights to our existing trained Model 2 (7:17)]
- [Preparing Model 3 (our first fine-tuned model) (20:26)]
- [Fitting and evaluating Model 3 (our first fine-tuned model) (7:45)]
- [Comparing our model's results before and after fine-tuning (10:26)]
- [Downloading and preparing data for our biggest experiment yet (Model 4) (6:24)]
- [Preparing our final modelling experiment (Model 4) (12:00)]
- [Fine-tuning Model 4 on 100% of the training data and evaluating its results (10:19)]
- [Comparing our modelling experiment results in TensorBoard (10:46)]
- [How to view and delete previous TensorBoard experiments (2:04)]
