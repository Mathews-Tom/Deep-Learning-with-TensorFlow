# Video Content

The course video content for the course can be found as part of the YouTube playlists from the channel [Artificial Whiteboard](https://www.youtube.com/channel/UCIslxZbyug9pRg00plukOCA) under the playlist [TensorFlow Developer Certificate](https://www.youtube.com/playlist?list=PLcPS3izl8N5sxZa7Ceu4tv8Y7P4TG_mXE).


## [Course Outline (5:21)](https://www.youtube.com/watch?v=D1kAy-pkNp4)    


## [00 - Deep Learning and TensorFlow Fundamentals](https://www.youtube.com/watch?v=YdqgT0IicIw)

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ Chapters ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬    
00:00:00 - 00.01 - What is deep learning? (4.38)    
00:04:38 - 00.02 - Why use deep learning? (9.38)    
00:14:17 - 00.03 - What are neural networks? (10.26)    
00:24:43 - 00.04 - What is deep learning already being used for? (8.36)    
00:33:21 - 00.05 - What is, and why use TensorFlow? (7.56)    
00:41:18 - 00.06 - What is a Tensor? (3.37)    
00:44:56 - 00.07 - What we're going to cover throughout the course (4.29)    
00:49:26 - 00.08 - How to approach this course (5.33)    
00:55:00 - 00.09 - Creating your first tensors with TensorFlow and tf.constant() (18.45)    
01:13:46 - 00.10 - Creating tensors with TensorFlow and tf.Variable() (7.07)    
01:20:54 - 00.11 - Creating random tensors with TensorFlow (9.40)    
01:30:35 - 00.12 - Shuffling the order of tensors (9.40)    
01:40:16 - 00.13 - Creating tensors from NumPy arrays (11.55)    
01:52:12 - 00.14 - Getting information from your tensors (tensor attributes) (11.57)    
02:04:10 - 00.15 - Indexing and expanding tensors (12.33)    
02:16:44 - 00.16 - Manipulating tensors with basic operations (5.34)    
02:22:19 - 00.17 - Matrix multiplication with tensors - Part 1 (11.53)    
02:34:14 - 00.18 - Matrix multiplication with tensors - Part 2 (13.29)    
02:47:44 - 00.19 - Matrix multiplication with tensors - Part 3 (10.03)    
02:57:47 - 00.20 - Changing the datatype of tensors (6.55)    
03:04:44 - 00.21 - Tensor aggregation (finding the min, max, mean & more) (9.49)    
03:14:13 - 00.22 - Tensor troubleshooting example (updating tensor data types) (6.13)    
03:20:47 - 00.23 - Finding the positional minimum and maximum of a tensor (argmin and argmax) (9.31)    
03:30:19 - 00.24 - Squeezing a tensor (removing all 1-dimension axes) (2.59)    
03:33:19 - 00.25 - One-hot encoding tensors (5.46)    
03:39:06 - 00.26 - Trying out more tensor math operations (4.47)    
03:43:54 - 00.27 - Exploring TensorFlow and NumPy's compatibility (5.43)    
03:49:37 - 00.28 - Making sure our tensor operations run really fast on GPUs (10.19)    


## [01 - Neural Network Regression with TensorFlow](https://www.youtube.com/watch?v=jLDq7yeLVQ0)

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ Chapters ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬    
00:00:00 - 01.01 - Introduction to Neural Network Regression with TensorFlow (7.33)    
00:07:34 - 01.02 - Inputs and outputs of a neural network regression model (8.59)    
00:16:33 - 01.03 - Anatomy and architecture of a neural network regression model (7.55)    
00:24:28 - 01.04 - Creating sample regression data (so we can model it) (12.46)    
00:37:16 - 01.05 - The major steps in modelling with TensorFlow (20.16)    
00:57:33 - 01.06 - Steps in improving a model with TensorFlow - Part 1 (6.02)    
00:07:34 - 01.07 - Steps in improving a model with TensorFlow - Part 2 (9.25)    
01:03:36 - 01.08 - Steps in improving a model with TensorFlow - Part 3 (12.33)    
01:13:02 - 01.09 - Evaluating a TensorFlow model - Part 1 ("visualize, visualize, visualize") (7.24)    
01:25:36 - 01.10 - Evaluating a TensorFlow model - Part 2 (the three datasets) (11.01)    
01:33:00 - 01.11 - Evaluating a TensorFlow model - Part 3 (getting a model summary) (17.18)    
01:44:03 - 01.12 - Evaluating a TensorFlow model - Part 4 (visualizing a model's layers) (7.14)    
02:01:23 - 01.13 - Evaluating a TensorFlow model - Part 5 (visualizing a model's predictions) (9.16)    
02:08:38 - 01.14 - Evaluating a TensorFlow model - Part 6 (common regression evaluation metrics) (8.05)    
02:17:55 - 01.15 - Evaluating a TensorFlow regression model - Part 7 (mean absolute error) (5.52)    
02:26:00 - 01.16 - Evaluating a TensorFlow regression model - Part 8 (mean square error) (3.18)    
02:31:54 - 01.17 - Setting up TensorFlow modelling experiments - Part 1 (start with a simple model) (13.50)    
02:49:04 - 01.18 - Setting up TensorFlow modelling experiments - Part 2 (increasing complexity) (11.29)    
03:00:36 - 01.19 - Comparing and tracking your TensorFlow modelling experiments (10.20)    
03:10:55 - 01.20 - How to save a TensorFlow model (8.19)    
03:19:16 - 01.21 - How to load and use a saved TensorFlow model (10.15)    
03:29:32 - 01.22 - (Optional) How to save and download files from Google Colab (6.18)    
03:35:51 - 01.23 - Putting together what we've learned - Part 1 (preparing a dataset) (13.31)    
03:49:23 - 01.24 - Putting together what we've learned - Part 2 (building a regression model) (13.20)    
04:02:45 - 01.25 - Putting together what we've learned - Part 3 (improving our regression model) (15.47)    
04:18:33 - 01.26 - Preprocessing data with feature scaling - Part 1 (what is feature scaling?) (9.34)    
04:28:08 - 01.27 - Preprocessing data with feature scaling - Part 2 (normalizing our data) (10.57)    
04:39:05 - 01.28 - Preprocessing data with feature scaling - Part 3 (fitting a model on scaled data) (7.40)    


## [02 - Neural Network Classification in TensorFlow](https://www.youtube.com/watch?v=aTw053kfu5Q)

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ Chapters ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬    
00:00:00 - 02.01 - Introduction to neural network classification in TensorFlow (8.25)    
00:08:26 - 02.02 - Example classification problems (and their inputs and outputs) (6.38)    
00:15:04 - 02.03 - Input and output tensors of classification problems (6.21)    
00:21:26 - 02.04 - Typical architecture of neural network classification models with TensorFlow (9.36)    
00:31:03 - 02.05 - Creating and viewing classification data to model (11.34)    
00:42:38 - 02.06 - Checking the input and output shapes of our classification data (4.38)    
00:47:16 - 02.07 - Building a not very good classification model with TensorFlow (12.10)    
00:59:28 - 02.08 - Trying to improve our not very good classification model (9.13)    
01:08:42 - 02.09 - Creating a function to view our model's not so good predictions (15.08)    
01:23:51 - 02.10 - Make our poor classification model work for a regression dataset (24.36)    
01:36:10 - 02.11 - Non-linearity - Part 1 (Straight lines and non-straight lines) (9.38)    
01:45:50 - 02.12 - Non-linearity - Part 2 (Building our first neural network with non-linearity) (5.47)    
01:51:38 - 02.13 - Non-linearity - Part 3 (Upgrading our non-linear model with more layers) (10.18)    
02:01:57 - 02.14 - Non-linearity - Part 4 (Modelling our non-linear data once and for all) (8.37)    
02:10:35 - 02.15 - Non-linearity - Part 5 (Replicating non-linear activation functions from scratch) (14.26)    
02:25:03 - 02.16 - Getting great results in less time by tweaking the learning rate (14.47)    
02:39:51 - 02.17 - Using the TensorFlow History object to plot a model's loss curves (6.11)    
02:46:04 - 02.18 - Using callbacks to find a model's ideal learning rate (17.32)    
03:03:37 - 02.19 - Training and evaluating a model with an ideal learning rate (9.20)    
03:12:58 - 02.20 - Introducing more classification evaluation methods (6.04)    
03:19:03 - 02.21 - Finding the accuracy of our classification model (4.17)    
03:23:21 - 02.22 - Creating our first confusion matrix (to see where our model is getting confused) (8.27)    
03:31:50 - 02.23 - Making our confusion matrix prettier (14.00)    
03:45:51 - 02.24 - Multi-class classification - Part 1 (Getting the data) (10.37)    
03:56:29 - 02.25 - Multi-class classification - Part 2 (Becoming one with the data) (7.07)    
04:03:38 - 02.26 - Multi-class classification - Part 3 (Building a multi-class classification model) (15.38)    
04:19:17 - 02.27 - Multi-class classification - Part 4 (Improving performance with normalization) (12.43)    
04:32:01 - 02.28 - Multi-class classification - Part 5 (Comparing normalized and non-normalised data) (4.13)    
04:36:15 - 02.29 - Multi-class classification - Part 6 (Finding the ideal learning rate) (10.38)    
04:46:54 - 02.30 - Multi-class classification - Part 7 (Evaluating our model) (13.16)    
05:00:11 - 02.31 - Multi-class classification - Part 8 (Creating a confusion matrix) (4.26)    
05:04:38 - 02.32 - Multi-class classification - Part 9 (Visualizing random model predictions) (10.42)    
05:15:20 - 02.33 - What "patterns" is our model learning? (15.33)    


## [03 - Computer Vision and Convolutional Neural Networks in TensorFlow](https://www.youtube.com/watch?v=i-P3w7QDN-8)

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ Chapters ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬    
00:00:00 - 03.01 - Introduction to Computer Vision with TensorFlow (9.36)    
00:09:36 - 03.02 - Introduction to Convolutional Neural Networks (CNNs) with TensorFlow (7.59)    
00:17:36 - 03.03 - Downloading an image dataset for our first Food Vision model (8.27)    
00:26:04 - 03.04 - Becoming One With Data - Part 1 (Inspecting the File paths) (5.05)    
00:31:10 - 03.05 - Becoming One With Data - Part 2 (12.26)    
00:43:37 - 03.06 - Becoming One With Data - Part 3 (4.22)    
00:48:00 - 03.07 - Building an end to end CNN Model (18.17)    
01:06:19 - 03.08 - Using a GPU to run our CNN model 5x faster (9.17)    
01:15:36 - 03.09 - Trying a non-CNN model on our image data (8.51)    
01:24:28 - 03.10 - Improving our non-CNN model by adding more layers (9.52)    
01:34:21 - 03.11 - Breaking our CNN model down - Part 1 (Becoming one with the data) (9.03)    
01:43:25 - 03.12 - Breaking our CNN model down - Part 2 (Preparing to load our data) (11.46)    
01:55:12 - 03.13 - Breaking our CNN model down - Part 3 (Loading our data with ImageDataGenerator) (9.54)    
02:05:07 - 03.14 - Breaking our CNN model down - Part 4 (Building a baseline CNN model (8.02)    
02:13:10 - 03.15 - Breaking our CNN model down - Part 5 (Looking inside a Conv2D layer) (15.20)    
02:28:32 - 03.16 - Breaking our CNN model down - Part 6 (Compiling and fitting our baseline CNN) (7.14)    
02:35:47 - 03.17 - Breaking our CNN model down - Part 7 (Evaluating our CNN's training curves) (11.45)    
02:47:33 - 03.18 - Breaking our CNN model down - Part 8 (Reducing overfitting with Max Pooling) (13.40)    
03:01:14 - 03.19 - Breaking our CNN model down - Part 9 (Reducing overfitting with data augmentation) (6.52)    
03:08:06 - 03.20 - Breaking our CNN model down - Part 10 (Visualizing our augmented data) (15.04)    
03:23:11 - 03.21 - Breaking our CNN model down - Part 11 (Training a CNN model on augmented data) (8.49)    
03:32:01 - 03.22 - Breaking our CNN model down - Part 12 (Discovering the power of shuffling data) (10.01)    
03:42:03 - 03.23 - Breaking our CNN model down - Part 13 (Exploring options to improve our model) (5.21)    
03:47:26 - 03.24 - Downloading a custom image to make predictions on (4.54)    
03:52:20 - 03.25 - Writing a helper function to load and preprocessing custom images (10.00)    
04:02:22 - 03.26 - Making a prediction on a custom image with our trained CNN (10.08)    
04:12:31 - 03.27 - Multi-class CNN's - Part 1 (Becoming one with the data) (14.59)    
04:27:31 - 03.28 - Multi-class CNN's - Part 2 (Preparing our data, turning it into tensors) (6.38)    
04:34:10 - 03.29 - Multi-class CNN's - Part 3 (Building a multi-class CNN model) (7.24)    
04:41:35 - 03.30 - Multi-class CNN's - Part 4 (Fitting a multi-class CNN model to the data) (6.02)    
04:47:38 - 03.31 - Multi-class CNN's - Part 5 (Evaluating our multi-class CNN model) (4.51)    
04:52:30 - 03.32 - Multi-class CNN's - Part 6 (Trying to fix overfitting by removing layers) (12.19)    
05:04:50 - 03.33 - Multi-class CNN's - Part 7 (Trying to fix overfitting with data augmentation) (11.45)    
05:16:37 - 03.34 - Multi-class CNN's - Part 8 (Things you could do to improve your CNN model) (4.23)    
05:21:01 - 03.35 - Multi-class CNN's - Part 9 (Making predictions with our model on custom images) (9.22)    
05:30:24 - 03.36 - Saving and loading our trained CNN model (6.21)    


## [04 - Transfer Learning in TensorFlow - Part 1 - Feature Extraction](https://www.youtube.com/watch?v=E799buwqA-k)

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ Chapters ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬    
00:00:00 - 04.01 - What is and, why use transfer learning? (10.12)    
00:10:13 - 04.02 - Downloading and preparing data for our first transfer learning model (14.39)    
00:24:53 - 04.03 - Introducing Callbacks in TensorFlow and making a callback to track our models (10.01)    
00:34:55 - 04.04 - Exploring the TensorFlow Hub website for pretrained models (9.51)    
00:44:47 - 04.05 - Building and compiling a TensorFlow Hub feature extraction model (14.00)    
00:58:48 - 04.06 - Blowing our previous models out of the water with transfer learning (9.13)    
01:08:02 - 04.07 - Plotting the loss curves of our ResNet feature extraction model (7.35)    
01:15:38 - 04.08 - Building and training a pretrained EfficientNet model on our data (9.42)    
01:25:21 - 04.09 - Different Types of Transfer Learning (11.40)    
01:37:02 - 04.10 - Comparing Our Model's Results (15.16)    


## [05 - Transfer Learning in TensorFlow - Part 2 (Fine-Tuning)](https://www.youtube.com/watch?v=biAIFUL7UuQ)

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ Chapters ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬    
00:00:00 - 05.01 - Introduction to Transfer Learning in TensorFlow - Part 2 (Fine-Tuning) (6.16)    
00:06:16 - 05.02 - Importing a script full of helper functions (and saving lots of space) (7.35)    
00:13:52 - 05.03 - (Excercise) Imposter Syndrome (2.55)    
00:16:48 - 05.04 - Downloading and turning our images into a TensorFlow BatchDataset (15.38)    
00:32:28 - 05.05 - Discussing the four (actually five) modelling experiments we're running (2.15)    
00:34:43 - 05.06 - Comparing the TensorFlow Keras Sequential API versus the Functional API (2.34)    
00:37:17 - 05.07 - Creating our first model with the TensorFlow Keras Functional API (11.38)    
00:48:57 - 05.08 - Compiling and fitting our first Functional API model (10.53)    
00:59:51 - 05.09 - Getting a feature vector from our trained model (13.39)    
01:13:31 - 05.10 - Drilling into the concept of a feature vector (a learned representation) (3.43)    
01:17:15 - 05.11 - Downloading and preparing the data for Model 1 (1 percent of training data) (9.51)    
01:27:07 - 05.12 - Building a data augmentation layer to use inside our model (12.06)    
01:39:14 - 05.13 - Visualizing what happens when images pass through our data augmentation layer (10.56)    
01:50:10 - 05.14 - Building Model 1 (with a data augmentation layer and 1% of training data) (15.55)    
02:06:06 - 05.15 - Building Model 2 (with a data augmentation layer and 10% of training data) (16.37)    
02:22:45 - 05.16 - Creating a ModelCheckpoint to save our model's weights during training (7.25)    
02:30:10 - 05.17 - Fitting and evaluating Model 2 (and saving its weights using ModelCheckpoint) (7.14)    
02:37:25 - 05.18 - Loading and comparing saved weights to our existing trained Model 2 (7.17)    
02:44:43 - 05.19 - Preparing Model 3 (our first fine-tuned model) (20.26)    
03:05:11 - 05.20 - Fitting and evaluating Model 3 (our first fine-tuned model) (7.45)    
03:12:57 - 05.21 - Comparing our model's results before and after fine-tuning (10.26)    
03:23:25 - 05.22 - Downloading and preparing data for our biggest experiment yet (Model 4) (6.24)    
03:29:50 - 05.23 - Preparing our final modelling experiment (Model 4) (12.00)    
03:41:51 - 05.24 - Fine-tuning Model 4 on 100% of the training data and evaluating its results (10.19)    
03:52:11 - 05.25 - Comparing our modelling experiment results in TensorBoard (10.46)    
04:02:58 - 05.26 - How to view and delete previous TensorBoard experiments (2.04)    


## [06 - Transfer Learning in TensorFlow - Part 3 (Scaling Up)](https://www.youtube.com/watch?v=0KXMNLZd9ZY)

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ Chapters ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬    
00:00:00 - 06.01 - Introduction to Transfer Learning - Part 3 (Scaling Up) (6.19)    
00:06:20 - 06.02 - Getting helper functions ready and downloading data to model (13.34)    
00:19:55 - 06.03 - Outlining the model we're going to build and building a ModelCheckpoint callback (5.38)    
00:25:34 - 06.04 - Creating a data augmentation layer to use with our model (4.39)    
00:30:14 - 06.05 - Creating a headless EfficientNetB0 model with data augmentation built in (8.58)    
00:39:13 - 06.06 - Fitting and evaluating our biggest transfer learning model yet (7.56)    
00:47:10 - 06.07 - Unfreezing some layers in our base model to prepare for fine-tuning (11.28)    
00:58:39 - 06.08 - Fine-tuning our feature extraction model and evaluating its performance (8.23)    
01:07:31 - 06.09 - Saving and loading our trained model (6.25)    
01:13:30 - 06.10 - Downloading a pretrained model to make and evaluate predictions with (6.34)    
01:20:05 - 06.11 - Making predictions with our trained model on 25,250 test samples (12.46)    
01:32:52 - 06.12 - Unravelling our test dataset for comparing ground truth labels to predictions (6.05)    
01:38:57 - 06.13 - Confirming our model's predictions are in the same order as the test labels (5.17)    
01:44:15 - 06.14 - Creating a confusion matrix for our model's 101 different classes (12.07)    
01:56:23 - 06.15 - Evaluating every individual class in our dataset (14.16)    
02:10:41 - 06.16 - Plotting our model's F1-scores for each separate class (7.36)    
02:18:17 - 06.17 - Creating a function to load and prepare images for making predictions (12.08)    
02:30:27 - 06.18 - Making predictions on our test images and evaluating them (16.06)    
02:46:34 - 06.19 - Discussing the benefits of finding your model's most wrong predictions (6.09)    
02:52:44 - 06.20 - Writing code to uncover our model's most wrong predictions (11.16)    
03:04:01 - 06.21 - Plotting and visualizing the samples our model got most wrong (10.36)    
03:14:38 - 06.22 - Making predictions on and plotting our own custom images (9.49)    


## [07 - Milestone Project 1 - Food Vision Big™](https://www.youtube.com/watch?v=DHAliaKd-mk)

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ Chapters ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬    
00:00:00 - 07.01 - Introduction to Milestone Project 1 (Food Vision Big™) (5.44)    
00:05:44 - 07.02 - Making sure we have access to the right GPU for mixed precision training (10.17)    
00:16:02 - 07.03 - Getting helper functions ready (3.06)    
00:19:09 - 07.04 - Introduction to TensorFlow Datasets (TFDS) (12.03)    
00:31:13 - 07.05 - Exploring and becoming one with the data (Food101 from TensorFlow Datasets) (15.56)    
00:47:10 - 07.06 - Creating a preprocessing function to prepare our data for modelling (15.50)    
01:03:01 - 07.07 - Batching and preparing our datasets (to make them run fast) (13.47)    
01:16:49 - 07.08 - Exploring what happens when we batch and prefetch our data (6.49)    
01:23:39 - 07.09 - Creating modelling callbacks for our feature extraction model (7.14)    
01:30:54 - 07.10 - Turning on mixed precision training with TensorFlow (10.05)    
01:40:59 - 07.11 - Creating a feature extraction model capable of using mixed precision training (12.42)    
01:53:43 - 07.12 - Checking to see if our model is using mixed precision training layer by layer (7.56)    
02:01:40 - 07.13 - Training and evaluating a feature extraction model (Food Vision Big™) (10.19)    
02:12:00 - 07.14 - Introducing your Milestone Project 1 challenge (Build a model to beat DeepFood) (7.47)    


## [08 - NLP Fundamentals in TensorFlow](https://www.youtube.com/watch?v=d3sXx8rT9fo)

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ Chapters ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬    
00:00:00 - 08.01 - Introduction to Natural Language Processing (NLP) and Sequence Problems (12.51)    
00:12:52 - 08.02 - Example NLP inputs and outputs (7.22)    
00:20:15 - 08.03 - The typical architecture of a Recurrent Neural Network (RNN) (9.03)    
00:29:19 - 08.04 - Preparing a notebook for our first NLP with TensorFlow project (8.52)    
00:38:12 - 08.05 - Becoming one with the data and visualizing a text dataset (16.41)    
00:54:54 - 08.06 - Splitting data into training and validation sets (6.26)    
01:01:22 - 08.07 - Converting text data to numbers using tokenization and embeddings (overview) (9.22)    
01:10:45 - 08.08 - Setting up a TensorFlow TextVectorization layer to convert text to numbers (17.10)    
01:27:56 - 08.09 - Mapping the TextVectorization layer to text data and turning it into numbers (11.02)    
01:38:59 - 08.10 - Creating an Embedding layer to turn tokenized text into embedding vectors (12.27)    
01:51:27 - 08.11 - Discussing the various modelling experiments we're going to run (8.57)    
02:00:26 - 08.12 - Model 0: Building a baseline model to try and improve upon (9.25)    
02:09:52 - 08.13 - Creating a function to track and evaluate our model's results (12.14)    
02:22:07 - 08.14 - Model 1: Building, fitting and evaluating our first deep model on text data (20.51)    
02:42:59 - 08.15 - Visualizing our model's learned word embeddings with TensorFlow's projector tool (20.43)    
03:03:44 - 08.16 - High-level overview of Recurrent Neural Networks (RNNs) + where to learn more (9.34)    
03:13:19 - 08.17 - Model 2: Building, fitting and evaluating our first TensorFlow RNN model (LSTM) (18.16)    
03:31:37 - 08.18 - Model 3: Building, fitting and evaluating a GRU-cell powered RNN (16.56)    
03:48:34 - 08.19 - Model 4: Building, fitting and evaluating a bidirectional RNN model (19.34)    
04:08:10 - 08.20 - Discussing the intuition behind Conv1D neural networks for text and sequences (19.31)    
04:27:43 - 08.21 - Model 5: Building, fitting and evaluating a 1D CNN for text (9.57)    
04:37:41 - 08.22 - Using TensorFlow Hub for pretrained word embeddings (transfer learning for NLP) (13.45)    
04:51:27 - 08.23 - Model 6: Building, training and evaluating a transfer learning model for NLP (10.45)    
05:02:13 - 08.24 - Preparing subsets of data for model 7 (same as model 6 but 10% of data) (10.52)    
05:13:06 - 08.25 - Model 7: Building, training and evaluating a transfer learning model on 10% data (10.04)    
05:23:11 - 08.26 - Fixing our data leakage issue with model 7 and retraining it (13.42)    
05:36:54 - 08.27 - Comparing all our modelling experiments evaluation metrics (13.14)    
05:50:10 - 08.28 - Uploading our model's training logs to TensorBoard and comparing them (11.14)    
06:01:25 - 08.29 - Uploading our model's training logs to TensorBoard and comparing them (11.14)    
06:11:51 - 08.30 - Downloading a pretrained model and preparing data to investigate predictions (13.24)    
06:25:17 - 08.31 - Visualizing our model's most wrong predictions (8.28)    
06:33:46 - 08.32 - Making and visualizing predictions on the test dataset (8.27)    
06:42:14 - 08.33 - Understanding the concept of the speed/score tradeoff (15.01)    


## [09 - Milestone Project 2 - SkimLit](https://www.youtube.com/watch?v=RW90m0u95Po)

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ Chapters ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬    
00:00:00 - 09.01 - Introduction to Milestone Project 2. SkimLit (14.20)    
00:14:21 - 09.02 - What we're going to cover in Milestone Project 2 (NLP for medical abstracts) (7.22)    
00:21:43 - 09.03 - SkimLit inputs and outputs (11.02)    
00:32:46 - 09.04 - Setting up our notebook for Milestone Project 2 (getting the data) (14.58)    
00:47:45 - 09.05 - Setting up our notebook for Milestone Project 2 (getting the data) (14.58)    
01:01:04 - 09.06 - Writing a preprocessing function to structure our data for modelling (19.50)    
01:20:56 - 09.07 - Performing visual data analysis on our preprocessed text (7.55)    
01:28:52 - 09.08 - Turning our target labels into numbers (ML models require numbers) (13.15)    
01:42:08 - 09.09 - Model 0: Creating, fitting and evaluating a baseline model for SkimLit (9.25)    
01:51:34 - 09.10 - Preparing our data for deep sequence models (9.55)    
02:01:30 - 09.11 - Creating a text vectorizer to map our tokens (text) to numbers (14.07)    
02:15:39 - 09.12 - Creating a custom token embedding layer with TensorFlow (9.14)    
02:24:53 - 09.13 - Creating fast loading dataset with the TensorFlow tf.data API (9.49)    
02:34:44 - 09.14 - Model 1: Building, fitting and evaluating a Conv1D with token embeddings (17.21)    
02:52:06 - 09.15 - Preparing a pretrained embedding layer from TensorFlow Hub for Model 2 (10.53)    
03:03:00 - 09.16 - Model 2: Building, fitting and evaluating a Conv1D model with token embeddings (11.30)    
03:14:32 - 09.17 - Creating a character-level tokenizer with TensorFlow's TextVectorization layer (23.24)    
03:37:57 - 09.18 - Creating a character-level embedding layer with tf.keras.layers.Embedding (7.44)    
03:45:42 - 09.19 - Model 3: Building, fitting and evaluating a Conv1D model on character embeddings (13.45)    
03:59:29 - 09.20 - Discussing how we're going to build Model 4 (character + token embeddings) (6.04)    
04:05:34 - 09.21 - Model 4: Building a multi-input model (hybrid token + character embeddings) (15.36)    
04:21:11 - 09.22 - Model 4: Plotting and visually exploring different data inputs (7.32)    
04:28:44 - 09.23 - Crafting multi-input fast loading tf.data datasets for Model 4 (8.41)    
04:37:26 - 09.24 - Model 4: Building, fitting and evaluating a hybrid embedding model (13.18)    
04:50:45 - 09.25 - Model 5. Adding positional embeddings via feature engineering (overview) (7.18)    
04:58:03 - 09.26 - Encoding the line number feature to used with Model 5 (12.25)    
05:10:30 - 09.27 - Encoding the total lines feature to be used with Model 5 (7.56)    
05:19:19 - 09.28 - Model 5: Building the foundations of a tribrid embedding model (9.19)    
05:27:46 - 09.29 - Model 5: Completing the build of a tribrid embedding model for sequences (14.08)    
05:41:56 - 09.30 - Visually inspecting the architecture of our tribrid embedding model (10.25)    
05:52:21 - 09.31 - Creating multi-level data input pipelines for Model 5 with the tf.data API (9.00)    
06:01:22 - 09.32 - Bringing SkimLit to life!!! (fitting and evaluating Model 5) (10.35)    
06:11:58 - 09.33 - Comparing the performance of all of our modelling experiments (9.36)    
06:21:35 - 09.34 - Comparing the performance of all of our modelling experiments (9.36)    
06:29:24 - 09.35 - Congratulations and your challenge before heading to the next module (12.33)    


## [10 - Time Series Fundamentals in TensorFlow and Milestone Project 3 - BitPredict](https://www.youtube.com/watch?v=vGnBXqL6KQ4)    

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ Chapters ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬    
00:00:00 - 10.01 - Introduction to Milestone Project 3 (BitPredict) & where you can get help (3.53)    
00:03:53 - 10.02 - What is a time series problem and example forecasting problems at Uber (7.45)    
00:11:40 - 10.03 - Example forecasting problems in daily life (4.52)    
00:16:32 - 10.04 - What can be forecasted? (7.57)    
00:24:31 - 10.05 - What we're going to cover (broadly) (2.35)    
00:27:06 - 10.06 - Time series forecasting inputs and outputs (8.55)    
00:36:03 - 10.07 - Downloading and inspecting our Bitcoin historical dataset (14.58)    
00:51:02 - 10.08 - Different kinds of time series patterns & different amounts of feature variables (7.39)    
00:58:41 - 10.09 - Visualizing our Bitcoin historical data with pandas (4.52)    
01:03:34 - 10.10 - Reading in our Bitcoin data with Python's CSV module (10.58)    
01:14:33 - 10.11 - Formatting data Part 1: Creating train and test splits for time series (the wrong way) (8.37)    
01:23:11 - 10.12 - Formatting data Part 1: Creating train and test splits for time series (the right way) (7.12)    
01:30:24 - 10.13 - Creating a plotting function to visualize our time series data (7.57)    
01:38:21 - 10.14 - Discussing the various modelling experiments were going to be running (9.11)    
01:47:34 - 10.15 - Model 0: Making and visualizing a naive forecast model (12.16)    
01:59:51 - 10.16 - Discussing some of the most common time series evaluation metrics (11.11)    
02:11:03 - 10.17 - Implementing MASE with TensorFlow (9.38)    
02:20:42 - 10.18 - Creating a function to evaluate our model's forecasts with various metrics (10.11)    
02:30:53 - 10.19 - Discussing other non-TensorFlow kinds of time series forecasting models (5.06)    
02:36:00 - 10.20 - Formatting data Part 2: Creating a function to label our windowed time series (13.01)    
02:49:03 - 10.21 - Discussing the use of windows and horizons in time series data (7.50)    
02:56:54 - 10.22 - Writing a preprocessing function to turn time series data into windows & labels (23.35)    
03:20:31 - 10.23 - Turning our windowed time series data into training and test sets (10.01)    
03:30:32 - 10.24 - Creating a modelling checkpoint callback to save our best performing model (7.25)    
03:37:58 - 10.25 - Model 1: Building, compiling and fitting a deep learning model on Bitcoin data (16.58)    
03:54:58 - 10.26 - Creating a function to make predictions with our trained models (14.02)    
04:09:01 - 10.27 - Model 2: Building, fitting and evaluating a deep model with a larger window size-27 (17.43)    
04:26:46 - 10.28 - Model 3: Building, fitting and evaluating a model with a larger horizon size (13.15)    
04:40:02 - 10.29 - Adjusting the evaluation function to work for predictions with larger horizons (8.34)    
04:48:37 - 10.30 - Model 3: Visualizing the results (8.44)    
04:57:21 - 10.31 - Comparing our modelling experiments so far and discussing autocorrelation (9.44)    
05:07:06 - 10.32 - Preparing data for building a Conv1D model (13.21)    
05:20:38 - 10.33 - Model 4: Building, fitting and evaluating a Conv1D model on our Bitcoin data (14.51)    
05:35:21 - 10.34 - Model 5: Building, fitting and evaluating a LSTM (RNN) model on our Bitcoin data (16.05)    
05:51:27 - 10.35 - Investigating how to turn our univariate time series into multivariate (13.52)    
06:05:21 - 10.36 - Creating and plotting a multivariate time series with BTC price and block reward (12:12)    
06:17:34 - 10.37 - Preparing our multivariate time series for a model (13.37)    
06:31:12 - 10.38 - Model 6: Building, fitting and evaluating a multivariate time series model (9.25)    
06:40:38 - 10.39 - Model 7: Discussing what we're going to be doing with the N-BEATS algorithm (9.39)    
06:50:17 - 10.40 - Model 7: Replicating the N-BEATS basic block with TensorFlow layer subclassing (18:38)    
07:08:57 - 10.41 - Model 7: Testing our N-BEATS block implementation with dummy data inputs (15.02)    
07:24:00 - 10.42 - Model 7: Creating a performant data pipeline for the N-BEATS model with tf.data (14.09)    
07:38:10 - 10.43 - Model 7: Setting up hyperparameters for the N-BEATS algorithm (8.50)    
07:47:01 - 10.44 - Model 7: Getting ready for residual connections (12.55)    
07:59:58 - 10.45 - Model 7: Outlining the steps we're going to take to build the N-BEATS model (10.05)    
08:10:04 - 10.46 - Model 7: Putting together the pieces of the puzzle of the N-BEATS model (22.22)    
08:32:28 - 10.47 - Model 7: Plotting the N-BEATS algorithm we've created and admiring its beauty (6.46)    
08:39:15 - 10.48 - Model 8: Ensemble model overview (4.43)    
08:43:59 - 10.49 - Model 8: Building, compiling and fitting an ensemble of models (20.04)    
09:04:04 - 10.50 - Model 8: Making and evaluating predictions with our ensemble model (16.09)    
09:20:51 - 10.51 - Discussing the importance of prediction intervals in forecasting (12.56)    
09:33:11 - 10.52 - Getting the upper and lower bounds of our prediction intervals (7.57)    
09:41:09 - 10.53 - Plotting the prediction intervals of our ensemble model predictions (13.02)    
09:54:12 - 10.54 - (Optional) Discussing the types of uncertainty in machine learning (13.41)    
10:07:54 - 10.55 - Model 9: Preparing data to create a model capable of predicting into the future (8.24)    
10:16:20 - 10.56 - Model 9: Building, compiling and fitting a future predictions model (5.01)    
10:21:21 - 10.57 - Model 9: Discussing what's required for our model to make future predictions (8.30)    
10:29:52 - 10.58 - Model 9: Creating a function to make forecasts into the future (12.08)    
10:42:01 - 10.59 - Model 9: Plotting our model's future forecasts (13.09)    
10:55:11 - 10.60 - Model 10: Introducing the turkey problem and making data for it (14.15)    
11:09:27 - 10.61 - Model 10: Building a model to predict on turkey data (why forecasting is BS) (13.38)    
11:23:06 - 10.62 - Comparing the results of all of our models and discussing where to go next (12.59)    


## [11 - Passing the TensorFlow Developer Certificate Exam](https://www.youtube.com/watch?v=1obZ9AZZYBw)    

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ Chapters ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬    
00:00:00 - 11.01 - What is the TensorFlow Developer Certification? (5.28)    
00:05:29 - 11.02 - Why the TensorFlow Developer Certification? (6.57)    
00:12:26 - 11.03 - How to prepare (your brain) for the TensorFlow Developer Certification (8.14)    
00:20:41 - 11.04 - How to prepare (your computer) for the TensorFlow Developer Certification (12.43)    
00:33:25 - 11.05 - What to do after the TensorFlow Developer Certification exam (2.13)    
